{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 3.0.32 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mjupyter-console 6.4.3 has requirement jupyter-client>=7.0.0, but you'll have jupyter-client 5.2.4 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is already saved in the Workspace and can be accessed at the file path provided below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name=\"/data/Tennis_Linux_NoVis/Tennis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -6.65278625 -1.5        -0.          0.\n",
      "  6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):                                         # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        #print(\"Action: \",actions)\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        #print(\"Rewards: \",rewards)\n",
    "        #print(\"dones: \",dones)\n",
    "        #print(\"States 0: \",next_states[0])\n",
    "        #print(\"States 1: \",next_states[1])\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  A few **important notes**:\n",
    "- When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "- To structure your work, you're welcome to work directly in this Jupyter notebook, or you might like to start over with a new file!  You can see the list of files in the workspace by clicking on **_Jupyter_** in the top left corner of the notebook.\n",
    "- In this coding environment, you will not be able to watch the agents while they are training.  However, **_after training the agents_**, you can download the saved model weights to watch the agents on your own machine! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "\n",
    "from ddpg_Multi_Agent import Multi_Agent\n",
    "\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "maddpg = Multi_Agent(state_size, action_size, num_agents, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_max_hist = []\n",
    "scores_mean_hist = []\n",
    "\n",
    "def maddpg_train(n_episodes=2000):\n",
    "    \n",
    "    scores_deque = deque(maxlen=100)\n",
    "    \n",
    "    for i_episode in range(n_episodes):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations\n",
    "        scores = np.zeros(num_agents)\n",
    "        maddpg.reset()\n",
    "        \n",
    "        while True:\n",
    "            action = maddpg.act(state, i_episode, add_noise=True)\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            \n",
    "            next_state = env_info.vector_observations\n",
    "            reward = env_info.rewards\n",
    "            done = env_info.local_done\n",
    "            \n",
    "            scores += reward\n",
    "            \n",
    "            maddpg.step(i_episode, state, action, reward, next_state, done)\n",
    "            \n",
    "            if np.any(done):\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "            \n",
    "        score_max = np.max(scores)\n",
    "        scores_deque.append(score_max)\n",
    "        score_mean = np.mean(scores_deque)\n",
    "        \n",
    "        scores_max_hist.append(score_max)\n",
    "        scores_mean_hist.append(score_mean)\n",
    "\n",
    "        print('\\r{} episode\\tmoving avg score {:.5f}\\tmax score {:.5f}'.format(i_episode, np.mean(scores_deque), score_max), end='')\n",
    "        if score_mean >= 0.5:\n",
    "            print('\\nEnvironment solved after {} episodes with the average score {}\\n'.format(i_episode, score_mean))\n",
    "            maddpg.save()\n",
    "            \n",
    "        if i_episode % 50 == 0:\n",
    "            print()\n",
    "            \n",
    "        if i_episode % 10 == 0 and score_mean >= 0.5:\n",
    "            maddpg.save()\n",
    "            \n",
    "        if score_mean >= 1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 episode\tcurr score 0.10000\tmoving avg score 0.10000\tmax score 0.10000\n",
      "100 episode\tcurr score 0.00000\tmoving avg score 0.01060\tmax score 0.00000\n",
      "200 episode\tcurr score 0.00000\tmoving avg score 0.01180\tmax score 0.00000\n",
      "300 episode\tcurr score 0.00000\tmoving avg score 0.01960\tmax score 0.00000\n",
      "400 episode\tcurr score 0.10000\tmoving avg score 0.01680\tmax score 0.10000\n",
      "500 episode\tcurr score 0.00000\tmoving avg score 0.01170\tmax score 0.00000\n",
      "600 episode\tcurr score 0.00000\tmoving avg score 0.06290\tmax score 0.00000\n",
      "700 episode\tcurr score 0.10000\tmoving avg score 0.10490\tmax score 0.10000\n",
      "800 episode\tcurr score 0.10000\tmoving avg score 0.14770\tmax score 0.10000\n",
      "900 episode\tcurr score 0.10000\tmoving avg score 0.32160\tmax score 0.10000\n",
      "941 episode\tcurr score 2.20000\tmoving avg score 0.52160\tmax score 2.20000\n",
      "Environment solved after 941 episodes with the average score 0.5216000077873468\n",
      "\n",
      "990 episode\tcurr score 1.00000\tmoving avg score 1.11750\tmax score 1.00000"
     ]
    }
   ],
   "source": [
    "maddpg_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 episode\tmoving avg score 0.00000\tmax score 0.00000\n",
      "50 episode\tmoving avg score 0.02627\tmax score 0.00000\n",
      "100 episode\tmoving avg score 0.01920\tmax score 0.00000\n",
      "150 episode\tmoving avg score 0.01660\tmax score 0.00000\n",
      "200 episode\tmoving avg score 0.02250\tmax score 0.00000\n",
      "250 episode\tmoving avg score 0.01860\tmax score 0.00000\n",
      "300 episode\tmoving avg score 0.01090\tmax score 0.00000\n",
      "350 episode\tmoving avg score 0.01270\tmax score 0.00000\n",
      "400 episode\tmoving avg score 0.01430\tmax score 0.00000\n",
      "450 episode\tmoving avg score 0.01330\tmax score 0.00000\n",
      "500 episode\tmoving avg score 0.01670\tmax score 0.00000\n",
      "550 episode\tmoving avg score 0.02750\tmax score 0.00000\n",
      "600 episode\tmoving avg score 0.04800\tmax score 0.10000\n",
      "650 episode\tmoving avg score 0.06410\tmax score 0.00000\n",
      "700 episode\tmoving avg score 0.07090\tmax score 0.10000\n",
      "750 episode\tmoving avg score 0.08550\tmax score 0.10000\n",
      "800 episode\tmoving avg score 0.10600\tmax score 0.10000\n",
      "850 episode\tmoving avg score 0.12170\tmax score 0.09000\n",
      "900 episode\tmoving avg score 0.12320\tmax score 0.40000\n",
      "950 episode\tmoving avg score 0.13500\tmax score 0.10000\n",
      "1000 episode\tmoving avg score 0.15610\tmax score 0.20000\n",
      "1050 episode\tmoving avg score 0.21820\tmax score 0.10000\n",
      "1100 episode\tmoving avg score 0.42440\tmax score 1.50000\n",
      "1105 episode\tmoving avg score 0.50330\tmax score 1.10000\n",
      "Environment solved after 1105 episodes with the average score 0.5033000075258315\n",
      "\n",
      "1106 episode\tmoving avg score 0.52830\tmax score 2.60000\n",
      "Environment solved after 1106 episodes with the average score 0.5283000078983605\n",
      "\n",
      "1107 episode\tmoving avg score 0.52840\tmax score 0.10000\n",
      "Environment solved after 1107 episodes with the average score 0.5284000078961253\n",
      "\n",
      "1108 episode\tmoving avg score 0.54050\tmax score 1.30000\n",
      "Environment solved after 1108 episodes with the average score 0.540500008072704\n",
      "\n",
      "1109 episode\tmoving avg score 0.55350\tmax score 1.80000\n",
      "Environment solved after 1109 episodes with the average score 0.5535000082664192\n",
      "\n",
      "1110 episode\tmoving avg score 0.57250\tmax score 2.30000\n",
      "Environment solved after 1110 episodes with the average score 0.5725000085495412\n",
      "\n",
      "1111 episode\tmoving avg score 0.59650\tmax score 2.50000\n",
      "Environment solved after 1111 episodes with the average score 0.5965000089071691\n",
      "\n",
      "1112 episode\tmoving avg score 0.61750\tmax score 2.60000\n",
      "Environment solved after 1112 episodes with the average score 0.6175000092200935\n",
      "\n",
      "1113 episode\tmoving avg score 0.61450\tmax score 0.00000\n",
      "Environment solved after 1113 episodes with the average score 0.61450000917539\n",
      "\n",
      "1114 episode\tmoving avg score 0.61740\tmax score 0.59000\n",
      "Environment solved after 1114 episodes with the average score 0.6174000092223286\n",
      "\n",
      "1115 episode\tmoving avg score 0.61440\tmax score 0.10000\n",
      "Environment solved after 1115 episodes with the average score 0.6144000091776252\n",
      "\n",
      "1116 episode\tmoving avg score 0.62040\tmax score 0.80000\n",
      "Environment solved after 1116 episodes with the average score 0.6204000092670321\n",
      "\n",
      "1117 episode\tmoving avg score 0.62240\tmax score 0.30000\n",
      "Environment solved after 1117 episodes with the average score 0.6224000092968345\n",
      "\n",
      "1118 episode\tmoving avg score 0.61930\tmax score 0.29000\n",
      "Environment solved after 1118 episodes with the average score 0.6193000092543661\n",
      "\n",
      "1119 episode\tmoving avg score 0.62430\tmax score 0.60000\n",
      "Environment solved after 1119 episodes with the average score 0.624300009328872\n",
      "\n",
      "1120 episode\tmoving avg score 0.62330\tmax score 0.10000\n",
      "Environment solved after 1120 episodes with the average score 0.6233000093139708\n",
      "\n",
      "1121 episode\tmoving avg score 0.62830\tmax score 0.90000\n",
      "Environment solved after 1121 episodes with the average score 0.6283000093884766\n",
      "\n",
      "1122 episode\tmoving avg score 0.65330\tmax score 2.60000\n",
      "Environment solved after 1122 episodes with the average score 0.6533000097610057\n",
      "\n",
      "1123 episode\tmoving avg score 0.65230\tmax score 0.00000\n",
      "Environment solved after 1123 episodes with the average score 0.6523000097461045\n",
      "\n",
      "1124 episode\tmoving avg score 0.65030\tmax score 0.10000\n",
      "Environment solved after 1124 episodes with the average score 0.6503000097163022\n",
      "\n",
      "1125 episode\tmoving avg score 0.65830\tmax score 0.90000\n",
      "Environment solved after 1125 episodes with the average score 0.6583000098355114\n",
      "\n",
      "1126 episode\tmoving avg score 0.66130\tmax score 0.40000\n",
      "Environment solved after 1126 episodes with the average score 0.661300009880215\n",
      "\n",
      "1127 episode\tmoving avg score 0.65530\tmax score 0.40000\n",
      "Environment solved after 1127 episodes with the average score 0.6553000097908079\n",
      "\n",
      "1128 episode\tmoving avg score 0.65330\tmax score 0.40000\n",
      "Environment solved after 1128 episodes with the average score 0.6533000097610057\n",
      "\n",
      "1129 episode\tmoving avg score 0.65230\tmax score 0.60000\n",
      "Environment solved after 1129 episodes with the average score 0.6523000097461045\n",
      "\n",
      "1130 episode\tmoving avg score 0.65530\tmax score 0.50000\n",
      "Environment solved after 1130 episodes with the average score 0.6553000097908079\n",
      "\n",
      "1131 episode\tmoving avg score 0.64930\tmax score 0.30000\n",
      "Environment solved after 1131 episodes with the average score 0.649300009701401\n",
      "\n",
      "1132 episode\tmoving avg score 0.67330\tmax score 2.60000\n",
      "Environment solved after 1132 episodes with the average score 0.6733000100590288\n",
      "\n",
      "1133 episode\tmoving avg score 0.67130\tmax score 0.00000\n",
      "Environment solved after 1133 episodes with the average score 0.6713000100292266\n",
      "\n",
      "1134 episode\tmoving avg score 0.69730\tmax score 2.60000\n",
      "Environment solved after 1134 episodes with the average score 0.6973000104166567\n",
      "\n",
      "1135 episode\tmoving avg score 0.72130\tmax score 2.60000\n",
      "Environment solved after 1135 episodes with the average score 0.7213000107742846\n",
      "\n",
      "1136 episode\tmoving avg score 0.71930\tmax score 0.00000\n",
      "Environment solved after 1136 episodes with the average score 0.7193000107444822\n",
      "\n",
      "1137 episode\tmoving avg score 0.73730\tmax score 1.90000\n",
      "Environment solved after 1137 episodes with the average score 0.7373000110127031\n",
      "\n",
      "1138 episode\tmoving avg score 0.74430\tmax score 1.00000\n",
      "Environment solved after 1138 episodes with the average score 0.7443000111170113\n",
      "\n",
      "1139 episode\tmoving avg score 0.76430\tmax score 2.60000\n",
      "Environment solved after 1139 episodes with the average score 0.7643000114150346\n",
      "\n",
      "1140 episode\tmoving avg score 0.76630\tmax score 0.30000\n",
      "Environment solved after 1140 episodes with the average score 0.7663000114448368\n",
      "\n",
      "1141 episode\tmoving avg score 0.79130\tmax score 2.60000\n",
      "Environment solved after 1141 episodes with the average score 0.7913000118173659\n",
      "\n",
      "1142 episode\tmoving avg score 0.81630\tmax score 2.60000\n",
      "Environment solved after 1142 episodes with the average score 0.8163000121898949\n",
      "\n",
      "1143 episode\tmoving avg score 0.83020\tmax score 1.69000\n",
      "Environment solved after 1143 episodes with the average score 0.8302000124007464\n",
      "\n",
      "1144 episode\tmoving avg score 0.83320\tmax score 0.50000\n",
      "Environment solved after 1144 episodes with the average score 0.8332000124454498\n",
      "\n",
      "1145 episode\tmoving avg score 0.85620\tmax score 2.60000\n",
      "Environment solved after 1145 episodes with the average score 0.8562000127881766\n",
      "\n",
      "1146 episode\tmoving avg score 0.85320\tmax score 0.00000\n",
      "Environment solved after 1146 episodes with the average score 0.853200012743473\n",
      "\n",
      "1147 episode\tmoving avg score 0.87720\tmax score 2.60000\n",
      "Environment solved after 1147 episodes with the average score 0.877200013101101\n",
      "\n",
      "1148 episode\tmoving avg score 0.88120\tmax score 0.60000\n",
      "Environment solved after 1148 episodes with the average score 0.8812000131607056\n",
      "\n",
      "1149 episode\tmoving avg score 0.88820\tmax score 1.60000\n",
      "Environment solved after 1149 episodes with the average score 0.8882000132650137\n",
      "\n",
      "1150 episode\tmoving avg score 0.91320\tmax score 2.60000\n",
      "Environment solved after 1150 episodes with the average score 0.9132000136375428\n",
      "\n",
      "\n",
      "1151 episode\tmoving avg score 0.93720\tmax score 2.70000\n",
      "Environment solved after 1151 episodes with the average score 0.9372000139951706\n",
      "\n",
      "1152 episode\tmoving avg score 0.95720\tmax score 2.60000\n",
      "Environment solved after 1152 episodes with the average score 0.9572000142931938\n",
      "\n",
      "1153 episode\tmoving avg score 0.97620\tmax score 2.00000\n",
      "Environment solved after 1153 episodes with the average score 0.9762000145763159\n",
      "\n",
      "1154 episode\tmoving avg score 0.97430\tmax score 0.20000\n",
      "Environment solved after 1154 episodes with the average score 0.9743000145442784\n",
      "\n",
      "1155 episode\tmoving avg score 0.99930\tmax score 2.60000\n",
      "Environment solved after 1155 episodes with the average score 0.9993000149168074\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1156 episode\tmoving avg score 1.02130\tmax score 2.40000\n",
      "Environment solved after 1156 episodes with the average score 1.021300015244633\n",
      "\n"
     ]
    }
   ],
   "source": [
    "maddpg_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEKCAYAAAAMzhLIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xd4XNWZ+PHvO0XVkqtw77h3Y4wNAZwN2ISaBLKGJCSQTWgpZH8JAZJQQthAIJtNAiwlSwnggGkhxgZMs0OJARdw712WiyzZ6jOacn5/zJ3xSLqaImk0I+n9PI8ez5x77p1zNfJ976lXjDEopZRSzXGkuwBKKaUymwYKpZRSMWmgUEopFZMGCqWUUjFpoFBKKRWTBgqllFIxaaBQSikVkwYKpZRSMWmgUEopFZMr3QVIVp8+fcywYcPSXQyllOpQVq9efdQYU9SSfTtcoBg2bBirVq1KdzGUUqpDEZG9Ld1Xm56UUkrFpIFCKaVUTBoolFJKxdTh+ijs+Hw+iouL8Xg86S6KiiEnJ4dBgwbhdrvTXRSlVBI6RaAoLi6moKCAYcOGISLpLo6yYYyhrKyM4uJihg8fnu7iKKWS0CmanjweD71799YgkcFEhN69e2utT6kOqFMECkCDRAeg35FSHVOnCRRKKdUZ1fuDvLByP8Fg+h5b3Sn6KJRSqrN69J87+e+3t+F2CV+dNigtZdAaRQby+/3pLoJSKkOUVnsBOF7rS1sZNFC0kZqaGi644AKmTJnCxIkTWbhwIStXruT0009nypQpzJw5k6qqKjweD1dffTWTJk1i2rRpLFu2DICnnnqKr3/961x00UXMnTsXgPvvv59TTz2VyZMnc8cdd6Tz9JRSaZIJPXudrunp169tZFNJZZsec/yAQu64aELMPG+++SYDBgxgyZIlAFRUVDBt2jQWLlzIqaeeSmVlJbm5ufzpT38CYP369WzZsoW5c+eybds2AFasWMG6devo1asXb731Ftu3b+fTTz/FGMPFF1/M+++/z1lnndWm56aUUvFojaKNTJo0iXfeeYebb76ZDz74gH379tG/f39OPfVUAAoLC3G5XHz44YdceeWVAIwdO5ahQ4dGAsW5555Lr169AHjrrbd46623mDZtGtOnT2fLli1s3749PSenlOrSOl2NIt6df6qMHj2a1atX8/rrr3Prrbcyd+5c2+GgxjQ/ciE/P79BvltvvZVrr702JeVVSqlEaY2ijZSUlJCXl8e3vvUtfvazn/Hxxx9TUlLCypUrAaiqqsLv93PWWWexYMECALZt28a+ffsYM2ZMk+PNmzePJ554gurqagAOHDjAkSNH2u+ElFJtot4fpKLWR5XnRGf08dp66v3BSFq1148vEASg5HgdRyo97Cur5VCFh6PV9QCU19S3f+Etna5GkS7r16/npptuwuFw4Ha7efjhhzHG8KMf/Yi6ujpyc3N55513uOGGG7juuuuYNGkSLpeLp556iuzs7CbHmzt3Lps3b2b27NkAdOvWjWeffZaTTjqpvU9NKdUKX390BWv3Hwdgz70XsK+slrPuXxbZvufeC5h4x1JOH9mbb80ayg0L1tge54H3djBlUA/OGd+3XcodTQNFG5k3bx7z5s1rkv7xxx83SXvqqaeapF111VVcddVVDdJuvPFGbrzxxrYqolIqDcJBImxveY1tvn/tLGNkUbeYx/p4V1laAoU2PSmlVDuK0U0Zlz9Ns7M1UCilVAfhDwbT8rkpCxQiMlhElonIZhHZKCJN2lBEZI6IVIjI59bP7akqj1JKdXSBNNUoUtlH4Qd+aoxZIyIFwGoRedsYs6lRvg+MMRemsBxKKZUxYl3qTcyt4A90sqYnY8xBY8wa63UVsBkYmKrPU0qpzi5dNYp26aMQkWHANOATm82zRWStiLwhIumZLaeUUhlA4qzs5OuETU8AiEg34GXgJ8aYxoswrQGGGmOqReR84FVglM0xrgGuARgyZEiKS6yUUpkp0Nk6swFExE0oSCwwxrzSeLsxptIYU229fh1wi0gfm3yPGWNmGGNmFBUVpbLIGSUQCKS7CAnRZdGVah+dro9CQgsdPQ5sNsb8oZk8/ax8iMhMqzxlqSpTKn3lK1/hlFNOYcKECTz22GMAPPzww/z85z+P5Hnqqaf40Y9+BMCzzz7LzJkzmTp1Ktdee20kKHTr1o3bb7+d0047jRUrVnDXXXdx6qmnMnHiRK655prIWlErV65k8uTJzJ49m5tuuomJEycCoeBy0003RZYnf/TRR5uU1W5J9PAxdVl0pVIr1npv8TqzO+OopzOAK4H1IvK5lfYLYAiAMeYR4DLgehHxA3XA5SbWbzERb9wCh9a36hBN9JsEX743ZpYnnniCXr16UVdXx6mnnsqll17KZZddxuzZs7nvvvsAWLhwIb/85S/ZvHkzCxcu5KOPPsLtdnPDDTewYMECvv3tb1NTU8PEiRO56667ABg/fjy33x4aNXzllVeyePFiLrroIq6++moee+wxTj/9dG655ZZIOR5//HG6d+/OypUr8Xq9nHHGGcydO5fhw4dH8tgtiV5fX8/8+fN1WXSlMli6JtylLFAYYz4kzjM3jDEPAg+mqgzt6c9//jN///vfAdi/fz/bt29n1qxZjBgxgo8//phRo0axdetWzjjjDB566CFWr14dWYK8rq4usoaT0+nk0ksvjRx32bJl3HfffdTW1lJeXs6ECRM488wzqaqq4vTTTwfgG9/4BosXLwZCy5OvW7eOl156CQgFge3btzcIFJMmTeJnP/sZN998MxdeeCFnnnkm69evb7IsOsCHH34YqQUluiw6QHV1Ndu3b9dAoVQS4nVmp2vCXedb6ynOnX8qLF++nHfeeYcVK1aQl5fHnDlz8Hg8AMyfP58XXniBsWPH8tWvfhURwRjDd77zHe65554mx8rJycHpdALg8Xi44YYbWLVqFYMHD+bOO+/E4/HErroawwMPPGC77lSY3ZLoX/nKV3RZdKXagc6j6KIqKiro2bMneXl5bNmypcFCgF/72td49dVXee6555g/fz4AX/rSl3jppZciy4aXl5ezd+/eJscNB5s+ffpQXV0dqSX07NmTgoKCyOc8//zzkX3mzZvHww8/jM8XWr5427Zt1NQ0XISs8ZLoa9asYezYsbosulIZrjP2UXQZ5513Ho888giTJ09mzJgxzJo1K7KtZ8+ejB8/nk2bNjFz5kwg1O9w9913M3fuXILBIG63m4ceeoihQ4c2OG6PHj34/ve/z6RJkxg2bFikWQhCfRHf//73yc/PZ86cOXTv3h2A733ve+zZs4fp06djjKGoqIhXX321wXHtlkTPyspi4cKFuiy6UhnMYVPrbw/S2r7j9jZjxgyzatWqBmmbN29m3LhxaSpRelRXV9OtW2hJ4nvvvZeDBw9GOp4zWVf8rlTXNuyWJZHXe+69gGVbj3D1kysbpIXzfGvWEJ79eF+zx3r+mlnMGtG7ReUQkdXGmBkt2VdrFB3UkiVLuOeee/D7/QwdOtT2GRdKqc4lPfUJDRQd1vz58yN9HkqpDqRjNeIAnagzu6M1oXVF+h0p1Tp2IxPbQ6cIFDk5OZSVlemFKIMZYygrKyMnJyfdRVFKJalTND0NGjSI4uJiSktL010UFUNOTg6DBg1KdzGUSqt4cyUyUacIFG63u8HMY6WU6ozS1PLUOZqelFJKpY4GCqWU6iDSNTxWA4VSSqmYNFAopVQ76oiDMzVQKKVUB6Gd2UoppTKSBgqllGpHf3h7Wyv21pnZSinV6W0sqWx2W6b2X2igUEqpdtJRlxnSQKGUUhkiXme1dmYrpVQnF69CkakVDg0USimlYtJAoZRS7aS1FQZdwkMppVRG0kChlFLtxG7UU3Ra/M5snUehlFJdmnZmK6WUaiJTg0M0DRRKKdVOtDO7EREZLCLLRGSziGwUkRtt8oiI/FlEdojIOhGZnqryKKVUJuoAFYqUPjPbD/zUGLNGRAqA1SLytjFmU1SeLwOjrJ/TgIetf5VSqtOxa2ZKZlmPTjcz2xhz0BizxnpdBWwGBjbKdgnwtAn5GOghIv1TVSallMo0HaFG0S59FCIyDJgGfNJo00Bgf9T7YpoGE0TkGhFZJSKrSktLU1VMpZRKKdMhwkJTKQ8UItINeBn4iTGm8fq6dhWpJr9JY8xjxpgZxpgZRUVFqSimUkqlRTKjnqQzPo9CRNyEgsQCY8wrNlmKgcFR7wcBJaksk1JKpYttH0UHqGWkctSTAI8Dm40xf2gm2yLg29bop1lAhTHmYKrKpJRSmaYjzKNI5ainM4ArgfUi8rmV9gtgCIAx5hHgdeB8YAdQC1ydwvIopVSHlq5RTykLFMaYD4kzP8SExoX9IFVlUEqpTNcRahQ6M1sppdqJ9lEopZSKacn6pl2wWqNQSikVcfeSTU3SOkCc0EChlFLtxe1sesnt0kt4KKWUaijLLlCkoRzJ0kChlFLtJMsV+5IbL2h0ypnZSimlTrCtUXSAKoUGCqWUaidul02NICpQpOvBRPFooFBKqXZi25kdFSniNj1pZ7ZSSnVu2vSklFIqJrvO7GTihNYolFKqk2vtPIp00UChlFLtROdRKKWUism26amrP+FOKaXUCS5H0wu9rh6rlFJdSDBo2H64qvnt9uuMZzwNFEop1Ub+d/kOzv2f99lYUmG7PdDKoKCjnpRSqoP7bN9xAEqOe2y3B4NNI0UHqFBooFBKqbYS747frukpuc7s9NBAoZRSbSTeRT9gW6PI/DqFBgqllGonNnFCl/BQSqmupEVNT214/FTRQKGUUm2suWU57JqeOgINFEop1WZi3/Lbd2YnVadIsjxtQwOFUkq1seYu/bad2R2gkqGBQiml2km8pqdMDRoaKJRSqo0110Bku4JHMvMoOltntog8ISJHRGRDM9vniEiFiHxu/dyeqrIopVR7arbpyXbU04m0dAWCeFwpPPZTwIPA0zHyfGCMuTCFZVBKqXYT70KvfRSNGGPeB8pTdXyllMpUzV38Wz2PomXFabWEA4WIfEFErrZeF4nI8Db4/NkislZE3hCRCW1wPKWUyljxhsdmau0ioaYnEbkDmAGMAZ4E3MCzwBmt+Ow1wFBjTLWInA+8Coxq5vOvAa4BGDJkSCs+Uiml0icYbJqW3MzszJ5H8VXgYqAGwBhTAhS05oONMZXGmGrr9euAW0T6NJP3MWPMDGPMjKKiotZ8rFJKpUy8y3i81WMztTM70UBRb0L1IwMgIvmt/WAR6SdWeBSRmVZZylp7XKWU6qg6dNMT8IKIPAr0EJHvA98F/hJrBxF5DpgD9BGRYuAOQk1WGGMeAS4DrhcRP1AHXG6Sm8uulFIZyv5SZn+FS/yyl64KR0KBwhjzexE5F6gk1E9xuzHm7Tj7XBFn+4OEhs8qpVSnEK/pyO7ZEx2h6SluoBARJ7DUGHMOEDM4KKWUap7tzOw426Nl7MxsY0wAqBWR7u1QHqWU6pB2H62hss4fM49dHOgIDe6J9lF4gPUi8jbWyCcAY8yPU1IqpZTqYL74++WR18lNuMv8SJFooFhi/SillGqp1i4KmKbu7EQ7s/8qIlnAaCtpqzHGl7piKaVU59Opm55EZA7wV2APoRFag0XkO9Z6TkoppaI0d+23mwHQmZqe/huYa4zZCiAio4HngFNSVTCllOpsWlujyNhRTxZ3OEgAGGO2YU2eU0oplZiO0MxkJ9EaxSoReRx4xnr/TWB1aoqklFKdk92op44g0UBxPfAD4MeE+ijeB/43VYVSSqmOLJl40BFiR6KBwgX8yRjzB4jM1s5OWamUUqoTsp+ZnfmRItE+ineB3Kj3ucA7bV8cpZTqvGxHPXWizuyc8LMjAKzXeakpklJKdU62o57avRTJSzRQ1IjI9PAbEZlBaGlwpZRSCbJtekqiSpGuJ9wl2kfxE+BFESkhFAAHAPNTViqllOrAmut3sF1mPNWFaQMxaxQicqqI9DPGrATGAgsBP/AmsLsdyqeUUp2GfY2i/cuRrHhNT48C9dbr2cAvgIeAY8BjKSyXUkp1WM1d/JurZyQqU59w5zTGlFuv5wOPGWNeBl4Wkc9TWzSllOpc4vdHZGb1Il6Nwiki4WDyJeC9qG2J9m8opVSX0lyfc2ubntI1PDbexf454J8icpTQKKcPAETkZKAixWVTSqkOKZmmp4ZpmfnQ7JiBwhjzXyLyLtAfeMucqDc5gB+lunBKKdWZtHbCXbrEbT4yxnxsk7YtNcVRSqmOr9nnUdilJTOPIk01jkQn3CmllGol+7Wemn+XKTRQKKVUK6wvrqDG608or90y452i6UkppZS9Gq+fix78kC+OKUpsh7irx8ZuWsr0RQGVUko1Uu8PArBm3/GE8htgTsygkpnVCw0USinVQuE7/MZNSs11UBtjmtYZkplHkXjWNqWBQimlWig8CinRfgZD0xVgg5lZiWggZYFCRJ4QkSMisqGZ7SIifxaRHSKyLnoZc6WU6kgSHeLaETqu7aSyRvEUcF6M7V8GRlk/1wAPp7AsSinV9iJNT4llD9o0PSX6KNS7XY+TvX1x4mVrQykLFMaY94HyGFkuAZ42IR8DPUSkf6rKo5RSqZLoxT7U9NQoLYFdc/HwLde7OMvTM9c5nX0UA4H9Ue+LrTSllMoYw25Zwm2v2ragRzqiPb5gg+QNByoYdssS1u4/bps/TlITw+UQAIGeoxLI3fbSGSjsOvBtf2cico2IrBKRVaWlpSkullJKNfTMx3tt05urSby7+Yj172Gb/A0vfYn0b8xybAYg0Gtk3LypkM5AUQwMjno/CCixy2iMecwYM8MYM6OoKMGJLUoplWLNXeMD1obGI5yMsWl6SuBzbnD9I3TcHiOSLWKbSGegWAR82xr9NAuoMMYcTGN5lFIqKc1d5MPzKpyORoEimYNYHATpI5V8EJiIZOUmXca2kLIlPETkOWAO0EdEioE7ADeAMeYR4HXgfGAHUAtcnaqyKKVUKjTXbBS0uiwaB4qWjHoaJKHm9kXB0xnXolK2XsoChTHmijjbDfCDVH2+UkqlWrwahd3aTMmu1zRSQi3yO4MDktuxDenMbKWUaqFm+yisiRWNnx/RkkehhgPFLtNfl/BQSqlM9sH2EyMu/YEgt/9jA4crPbZ5wzUKh12NIkbwKDne9HgjpYSjppDjFLSg1G1DA4VSSiXgysc/jbz+cMdRnl6xl5tfXmebNzxT22HTzhRr1NM/tzUd/j/OsY+dZoC1rz7hTimlOoTwxT3QzNodsfoomstrZ7ZjI1MdO1kWmJpsEduUBgqllGpjkT6KRGoUMQLF9c5FACwInNN2hWsBDRRKKZWsOB3QJtL01HRbIh3cYWc51wNQRZ61b3pooFBKqRaKP+opgWM0kz5SDgDwJ//Xki9YG9NAoZRSLdTcZLnwEh4O+ypFA831UVzi/AiAlwJnnthVn5mtlFJtr9LjY9WeWE88aHvhfge763rjtDV77Z+3PVl2szE4lP2mb9sWrgU0UCilOrXrn13NZY+soMbrb/NjN/fAomSeZPfER7tt04fJIXabzHhEjwYKpVSntr64AgBfIBgnZ/JiDW0F++GzicyFcOFnkJSy2/RruG+aurM1UCilVDOaG7qa6BPtAjbZErnUz3ZswiVB9gT7xc/cDjRQKKW6hJbcjcdtQoqzPZjow7QbOduxFoClwRkNN2hntlJKpU6itYCG+7RsezjdrmkqkZFLZznWsTo4impr/kS6aaBQSnVqrVkfKd5jSpvbHv7EgF2giPOZpzs2MNpxgCWBWQmUsH1ooFBKqWa0rOHohIBdJ0Uc5zlWAvDP4OQm23QehVJKpdDUu97mmqdXNUn/6QtrGXbLEtt94vVRxAsD//32tiZp8Wo4g6SUTcGh7DQD4xy9/WigUEp1GW9tOtwk7eU1xc3mj9evEW94rJ14lYIxjv3szoBJdtE0UCilVDOaiwMtiA9xFXGM2Y6NDJQyDptetnnStShgyp6ZrZRSnV2zgSTWTjZX+5tcz/MD16LI+xcDZ7eqXG1NaxRKKdWMuH0ULahZNJ7P4STQIEgAbDZD7PdNU2+21iiUUp1aa66tLZl7kYwcvPzJ/RAAP/d9nxcDZ5ONj/Q1MtnTQKGUUs1oaV9ErE7ucOC60vkWv3E/FUl/PzAZgwMP2S370BTSpielVIdnjOGOf2xg88HKNjne0Wov/2/h59TWBxqk/+2TfbwSNUrqwPG6ZsrT/LHDdYULnJ8A8JR/LhM8j3OI3nHLpU+4U0qpFjpc6eWvK/Zy1ZOftsnxfr90K698doBXPzvQIP0Xf1/P/3thbZuMehrIUV4LzOJO/1XUkNv6A6aQBgqllGpGKvooRCAPD4MdpWwLDkp633TQQKGU6jTaen5DKuZLCMLJ1vOwt5nkAkW6aKBQSnVqrbkJT9WYp8mOXQBsaWYYbKZJaaAQkfNEZKuI7BCRW2y2XyUipSLyufXzvVSWRynVubVqKKxN9SE1NQrDda7X2B8sYm+SS3Wk6wl3KRseKyJO4CHgXKAYWCkii4wxmxplXWiM+WGqyqGUUokw5kSgCQeIVPRRDPJsZZAc5YXA2WTafInmpLJGMRPYYYzZZYypB54HLknh5ymluoBlW49QW+9vkgah0U/lNfUALLfJF0vQGPaW1fC3T/ZRUmE/7LUtDK7dDMCf/F9L2We0tVQGioHA/qj3xVZaY5eKyDoReUlEBtsdSESuEZFVIrKqtLQ0FWVVSnUAO45UcfWTK/nl3zdE0naWVnPrK+sj77/xl4/ZWVrNVU+ubJAejwHOvn85v/j7ej7YfjSU1pK1nOIYWLeFo6aQA/RJet/OOOrJ7pQa/35fA4YZYyYD7wB/tTuQMeYxY8wMY8yMoqKiNi6mUqqjqPKEagi7jtZE0qo9DWsNWw9XRdJ2R+WLx242dbwn3LXE4LrNrAuOoKM0O0FqA0UxEF1DGASURGcwxpQZY7zW278Ap6SwPEqpTqjxpdwh0qI7fruY0NZxIg8PRZ69rDMj2vbAKZbKQLESGCUiw0UkC7gcaLBEooj0j3p7MbA5heVRSnVCje/6HXIiTUh8xVXbQJHgZyZqguzBQZC1wZEt2j9dUjbqyRjjF5EfAksBJ/CEMWajiNwFrDLGLAJ+LCIXA36gHLgqVeVRSnUNDQJDEo36yYxwCrawpjHFsRPAanrqOFK6eqwx5nXg9UZpt0e9vhW4NZVlUEp1bo2v2c4WNj3ZXfybrzgk+wmGMx3r+bHrFY5mDaLM0z3J/UM6Y2e2Ukq1mDEGfyCY9H6OOBdTfyBIIGgINooMds1J/qD95yfb8vQt5zs8k3UvhVLH/vyJye2cAfR5FEqpjPT8yv3c+sp6/nXLvzGgR+KrqzqibrvtYsbJv3wDgIIcF+vvnBdJt6tRPPDeDtvPSLbpaaZjCx7j5pHARUi/y+BgcvuHpWtmttYolFIZadHnoUGSe+yGuEbd0je+u3fEq1JYqhoNq02mNSmZ/oypsoMvOz7lk+A4/ui/jLKcoYl/UIbQQKGUymjxL8l2o55OvE/0HjzWU+mafGISQeX7rsW4JcCTgXlJlSeTaKBQSmWkxusu2W60EWp6MvGyNZFMa1IyeYfLYd4LTGV5cFoSe9nTzmyllIoSCRR2l+UEm55OhIz4kqtRJJZ3pBxgvGNvg+dOJDqvI5NoZ7ZSqsNI5PLcuOkp4WMn00cRN6/hZtfzXOF8D79xsDw4NfkC2dBnZiulVJTwCJ/oi3LkTj5u01M4W+LjhJKZbR2vM/srjo+43vUaB00vLqm/m4+D4xM+dibSQKFUF7XtcBU3LFiNL2quwvbDVVz3zGrq/YnNXzhS5eHqJz+lotbHjiPVXPfMat7ZdJjb/7Eh5n5LNx7it6+HVuxZuHIfDy3bwVsbD/GbxSceV3Oi6emEyLBUY3hmxR4e/3B3m6319PSKvQnn/c+Fa23Tc/DyfNZv+GPW/7Ir2I8L63/LRjOsQZ4O2PKkTU9KdVU/fWEt6w9UcO1ZlUwZ3AOAn7+8js/2HWf9gQpOGdoz7jEeWb6LZVtLeXH1ft7adJhPd5fz5sZDANx1SfMTy659ZjUAvzh/HDe/3HAp8NsubP7uO3qS3G3/2AjAwmtmNcjjcLSs6enBZfZzJhKVhY/fuf/CLMdmFvi/xJ/9XyWAs0m+WHWc2y8cz12LGz/bLWrfNEUZrVEo1UX5rYuuy9k2Fx93Gx2nsegmoYBNBGg8+c0hQiAqsb2urTe7nucS5784YHrzS/93OUyvpI8xdUiPyOtrz8qc9aA0UCjVRYWXx3A5TlwGWrOsttPR8HLSeImMZIX7Ghr2UTTN13i0kkOkweqxqXjudTQhyDmO1fyH6w12BftxmfdOYnU7xwpcjgxtl9KmJ6W6KH8rL+SNuRvNiPYHDVkJzpK2Yzc81m4Ia9NAcaKW0R7X3f9yPc43XMsA+LnvGg7SO2b+WEWKV1wd9aSUalfhBe+iO7Nbc2Ft3ITV3IJ60exGGsUafRSwCW62TU+mPZqeDF91fMA3XMvYEhzM6Z4/s8qMbdURM7RCoTUKpboqfyB0MY2++LammcbVqOnJF4h/MLtKTSBocDlPdPnGbXoKNm16Ckaantqo/wU/faWcYXKYibIbH06udi1lkBylyuRyne8nlCT4DOxYwSBeedMVSDRQKJUmGw5U4HIK5TX1DOudn9QKqdFW7SmnT7dshvXJ580NB1nwyT5++MWTOW1EbzYcqMDpEMb1L2ywz/Haeg5WeIATd/7GGD7ffzzyOhg0vLauhLNGFfHi6v2M6lvASQXZCML4AYW8sf4gdb4TC+s1rlHc9domrj17BG9vOozXH+Q/zxlFpcfPp7vLI3l8NsuIe/1BXltXEmkaMwaWbz3ChAHdI8/Kjm42+02jUUJbD1dx4FgdAJ/uKae1ijjO4uxf0FeON9n2F//5/N7/73jJSvh4sUYuNdiUQbULDRRKpcmFD3wYeV2Q7WL9r+fFyB3yl/d3cfJJ3fji2JMiaZc9sgKA3fecz3XPrgHgg+1H2XPvBZHP2HPvBQ2Oc72VD0J3/i+u2s+HO45G0vxBwyufHeBnL65lTN8Cth6uIsvpoN6PUrilAAAc6klEQVS6sC/+0Re4fsGaBsd0NuqPeHlNMS+vKY68nz6kB09+tId/bitt8DmNTbhjaYP3z36yl+VbSxukbSypjLzeZbO67K9ejT2PIxF5eBgoR3km6x76ynH+6j+XZcGp7DQDGCGHOGoK2WiGt/pzojUXQ26aN4b7l25t089KhgYKpTJAldcfPxPwX9YktcYXfkisqSdsd9TFNRA03PTSugbb/QHD0Wpvg7z1UXf/1TbldTtid3lW1PkafC5AIIEyNw4SbSXX3XSOA8AgKeV219PMda6OpD3V7xfcuefEvJCZ007hn1FBMBmxO7Ptt/7giyfzgy+e3KLPawsaKJTqJOp8gYTzRt+52jX/+KI6ousTfMpcS+Zj+BLo8E4Ng8PUgzEUUs05jjV0lxqmO7ZzkfNjakw27wWmsjo4mg+CkxjZ40ygJLK3qxWjuWJFCu3MVkq1uegRQt5kAkXUa7uRRPHu9O06lRs3PSXC7rPbmhBklmMzVzmXkoUPL1mMkmJGOg7C73qwLudE34PHuFkUmM3v/f/OPtM3kj7G2bC21FaTFJuUVQOFUqo17C6q0Xf7Hl/id+fRHap2TVbxhrba1TJactFvWJsx5FCPh+yY+3SnGidByilAMBgcDOAopzi2cZZjHaMdxeTjoY9U4MNJkZzoz6gzWQQRtpuB/J//fL43oS+LP93E+uAIFgVO5yC9sLvld7saBgq3s+UzC2KNbErXo07j0UChVAfhsakxRAcHj7/hdn+CTUZ2QSHeZLzGZTHmxHBbB0EmyB5Oc2xmrGM/w+Ugfagg//1BDPfUUZ8VwEEQAYqe68bbWaX0lXKy8JMjPj4LnkwfKgjgwI+TOrKoJYe+HKOObMY59gFwzHQjDw/V5NJbqgAIGGGtGck2M4i9pi815OAiQInpzeP+85tMhvuPC8/nhx+9Hvd31HgyYUtqT2GxZ2ZH5cugoNHlAkVdfYBslwOHQ6jx+snLclJeU09+tityR5SfHfq11PuDVHl89O4WusOp8vgoyHFHjlXj9UfyenwBXI7QRB9ByLLuQGq8fnLczshqnP5gkIIcN4GgwesPUOXx43IIvbtlc7y2niyXA6cjtFaNMaG1bQpz3NTW+8lyOnA5HXj9AerqAxTmuKkPBBFr/f0cq3MuXBaXdddjjKHa6ycvy0VZjZeCbHfkc8KMMRyp8tK3MAeA2no/OS5ng4fAhH93AWMIBA05bieBoKHeHyQ3yxnZzxcwOB1Ct2wXHl8Ap0Oa3IEFggZfIBgpc1gwaPBax/P4AoiE8np9QQpz3bb/Qf2BIH6rPBC6Sz1S5cXtEE6yzif8ewkagz9o6JblavJs5Yo6H5V1PnrlZ1Hl8VPvD5Kf7aQgx01dfYCAMeRlOTlwvI6CHBe+QPg4WHlDf0P52S4qan10z3PjDwQ5XOWlZ56byjo/PfLcVHn8dMtu+l+vyuOjyhP6mzpeW48vYCjqlk1OloPKOj9HqjyRvHuO1tAjz91gxM/mg5UNjrflUFXkdfGxWrJcDowJXeSO19ZHtpUcr2tSlp1HajhUGfq87lRzmmMz0x078OOggFoGfPI6t7gqqDY55ImXqevdjPXUcq77EDMdWymUWgAOml7sN0XsMAMZFXDgCTrxGicGwSBUOwvZb2BFcDw+XBhgpmMLa80ICqklS/zUmBwKpYbdph9BHKzzj6CKXLpRhw8XDutoa8woFgVOpx53k/NpzrbD1QnlazykVZueOrHDlR5O++27DOqZy8JrZ3PGve/x03NH899vb2NY7zz2lIX+uH998QS6Zbt49pO9fLbvOJvvOo9NByu49OEV3HHReGaP7M3WQ1Xc+PznvPmTMxnbr5Cxt73JGSf35qMdZQzskctHt/wbRyo9zPztu03K8cAV03hzwyGWrD8YSbtgcn+WrDvYIF/v/Cz8QcPaO+Yy/valnDW6iKe/O5Pxty8lEDQM6plL8bET/8nDI2HG3vYmM4f34oVrZwPw+7e28tCynXz9lEG8uDo0UuPfZwzivsumRPZ94qM9/GbxJv72/dOYPqQn429fyve+MJxfWSt5ev0Bxt3+Jv/xheH8a2cZmw9WsufeC7jtHxv42yf7WPqTs+hbmM3Uu96OHHP9nXOZdOdbkbJVeXy8ueEQ4wcU8vDynSxedzBS5g0HKuiR5+ax93fx9Iq97Pzt+Yy97c0Gv48LJ/fnwW9Mj7zfW1bD4x/u5rW1JRyr9UWONfd/3o+Mrvnoln9joDU/Ifp43zxtCP/11UkNjj/n/mUcq/U1+b6iTRxYyIYDlTHzXD9nJA8v38mZo/rwwfajMfNGC/+uEjHn98ubpN34/OcN3kcPv/3C75bZHseNnwdfX8VYKWeSYxfTZDvTHDvIet+PHyc3ZR+nl4Qupj7jxC0BKkwerr0BJrlCo6ICRqg8ko8XN1WSx5LAaawITmB1cBQHKDrxYYdtCrA74VNOiXl/fD+hfI0De36W/aUzfJMXS47LfrRVSGZGii4VKM6z/iiKj9XxX0tCk3T+++1tAJEgAXDHoo0N9qv0+CIX5F+/Ftrv4ikDgNBd3Nh+oclMH+0oA+CAdYdWbHOnBvCj5z5rktY4SACU1dQ3eP++Nf48/IcYHSQai57U9NCynQCRIAHwwqriBoEiPNHqcKWHuvpQs8LCVfsjgSKStnJ/g6GRf/sk1Aww74/v887/O7tBGcLHBPh4VxmPvb+L97YcaZDHGIOIRC5q4RqD19+0mWXxuoM8+I0T78++f7ntuUcPwSyt8kYCRbS/fbqvSaCIFySAuEEC4OHlod93MkGiLWRTzzA5xEzHFkZLMcPkEEVSwcCsGo7Wu9lpBmBwkE8d+eKhkBqGyhEccuLC5nfksMJ3MsfpRjY+VgdHc8TZj4svuZTvvR1k/7E6Jgzuw3e/MJyDx6r505vrMcDvrgjdlDhFcHh9zMt2M/5YLf2759C/ey4//NsajlR5uXjKABatLcHlEG67cDwPLtuB1xfgiplDePT9XTHP7/JTBzNrRG9eXL0/8n/tsStPYV95LXcvCQ0b/t9vTqespp7bWjCP4qIpA3htbQkXTOrPb74ykQ+2l1qzxB0M7JEbWYb84W9O54xRfbh4ygA2lFQwok83fIEgXn+Qk0/qRkWdD48vQN/CHA5VeKit93Os1kdelpNjtfWcOaqIS6YO4M/vbueVzw7wzdOGsMD6f5TtysxVlbpUoIi+EByIcZFtrK4+kNbn3Cba1hwMmibNKcnyBUJNP43ZpTXV8E6qtv7Exb68pp5NJU0vsr6AIcvVtMx19YmP4InFrl3fjt0Q0eaM61/YpJknOQYnQR64oB/n9SjmnuffxkmQQ6YXBVJLAXX0kGr6Sxk1JofjdCN8pzlQSulFlXUUMAjdpYbBUkpPOdGMUmHyKBw4BgomUevuyca1OxklxQRxUEUu5aaAfZzE0uBMrrvwDMjrAwOn4+o+mCt/daJW+KsLxvHjM4bjcAhnFW/kqX/t4aIpA7h4ygCCQcM9b24HTtw4NefLE/vx1xV7mTq4B7//+hQCQUNulpMrZw0FQs+5/vl5Yxn5i1B/wRdO7tNgAiDAd78wnNF9C+hbmMNHO8roW5jN3An9ACKB4pxxfclyORIOFJdOHxSZFPjAFdO479LJkWbUS6YOjOTbcKACgLH9CvjypP4AFOa4GdYnv8kxe+Vn2b6O1j3XTY+80LbhUcdo3BSbKbpUoGgpjz/Q7AVHaL6q2VbLG1d5EpuM5fUHyXG37I4kfKn2+uzPNZwWKwzV+xuecCIXaY8/EOnPaZie/Ph6fyAY6ZdJpgzJ5INQk2A8OXgZIGXk4mWklDBUDjPRsYdRUkyRVFAgdWC1Sv7Spknda1yUmN7kObx0pyb8UFCOUUCxKcIQ+i4EQx3ZLA7M4rQpExg1ejxnPu+h2PRhzzUXhs6t2ssPV73TbFmvm9V08l6YQyRy89H4XimZm5LwjZaBBt939DGi+5/s+gDCTTax/saTfSZG42OFg0RjqVj+2+5xqtkt/P+baikNFCJyHvAnwAn8nzHm3kbbs4GngVOAMmC+MWZPKsuUg5cgjqSWWPb4gjHHqNs1kwAJP04ynuN18ZtEIHSxizM5NoFjBJuMngmnx9230X7RTVSCfUedxxegwKZjtyU1Co8/SLcmgeLEOkYx901iaGlRLpwiWxnn2Ec2PlwEyMLHKMcBxsteBkgZeeJtst/uYF82mmGUBntQSR7nnTaFMafMYdKfQ00afaWcY6aAanLx4ibZ9upHx5/C6An9KH5uSYP0TLhLjSwZnuDdU+MFBuHERb25i3noc5L7naXz+Q/hX0V0GRo+77u9S9S8lAUKEXECDwHnAsXAShFZZIyJXsHrP4BjxpiTReRy4HfA/FSV6WQp5p3snwOwpOoctjm7s8qMZprsYKgcpo5ssvERQCiUWgRDH6lk8DsjEdOH21xHcOOnlhyGHO1Nf6eXYbs2Eqzvy/mOHXhxU48bHy4oGYipEIo4ThAhPCAwiIMgDuvy4sCNnxzqycKPGz9Z4sNNgCz8ZOHDjR//9iBfdKwlCz9mvYdzHRvoJVV0p5o88ZKLlx7UkLX0TUxuD653HqEeF3xaArk9mevYQD1ujptuOAhSRzYesqCmDLILwOmO3Nt4fAHbi6bdHXfj5prGeY5HNfUFjLGtYXl9QdtmrYpGwbEbtTgwEAzSXDT0+AJNOh3DQTze8hbhsufgpYA6ekoV42Qv4xz7GC976SlVuAhQIHX021GBK7tp8C4xvVgfHMHy4BTKTQElpg+1ZLPfnMQu07/JwnGTR5/KmAEnUWXN+K0yeTHLGE9z15WMCBRJBj27mc85VoCI3RnccUQerpRBAaE5qaxRzAR2GGN2AYjI88AlQHSguAS403r9EvCgiIhJ9LYjSXe7n4y8vsD3DhdEVfkrTR6FUovHuAnioI4s6simzBSSW7aBiTUljHUaasghHy/ZZT7OdwPrQz//27g14rG7OR1YmUPrLYUnw8d/Gf7S6LM8xk0F+eRsX4/DW8nNbusi9vqzoaI011Jy/08jL+929uLubA85/xLMZ714IwsMLvi/+yG7gIGBPH7rqqEQH353MHTRfnEhD7kPIoADw6j3CvmL+xj5eMiVek5a6ebMLA9Oggx8J5sFvjpysmtxE0CsFvaej7lxCHyW7bUGTIaaU/KedbA+O2ClGfLDd+h3Xw/uPHBm8W6WGy9Zofx4KPzrSZBbyKtZh8jHy2A5gnsR8JoDtzjYlB3EAFbp4d4sQEAc9DewMtvfYHIWhJqAtplBlJtCPGRRZfIY0H8IT+0rYm1wJHVk48OJH1foBiEDtWbMv52WHC3ZIsRqemrL5hm75p/2Ev7kDhAnUvqXPRDYH/W+GDituTzGGL+IVAC9gTYfLrJu+cvMcoQ6vIZ5FtCPcqY7tltVfHg3OB0XAfw2v5IBvhy84qfC64lsdxAkh3oGdYNebh/lx4+TjY8sfGSJn9HdDTn15dR4QhdAh9WBKQRxEiQHH04JUG/c1FoNGF7c+EzoglNvXXi8uCnMy+NQTYB63PTr2Y2jxyqoJI9jpoAqcjHW86eG5OfhyjUcKKsgCz/j+rgpMFUcKq8km3qKpIIggpMg2dQzpqCeHDxkmXryvYepDGThEjd9aqsxpg53wE/tQT/dzH7yTDVznTXUmmwCEqoVlWzdzygxkQvvsZJS+gvWBTWXmlonXpNDEKG0ykWdvzu15OA1LrBCQL7fBeKgIuCPjK0HyA24qAkEIx225aYQL25OzveQYzy4fT6yTak1M1eoI5uSci+5VHHMFHCQ3iwPTsHtzCbXGXo0ZmWgPvJdCIaeQTdiggiGoAniD9Syz5xEOYVUmVy2mCHsNv2bBIAfDB/JW3t2tvpv0tnGt5JtHRCgYZt/uGbSkmdjh/dNdI0ku+Gn4c8N/96aG6KajCxnYrWT8O82VrNX8p8d+n8bPes7+rfTeJJfOqUyUNidZePwnUgeROQa4BqAIUOGtKgwWfnd+TT3TP6vYgYgTJ80gTc39Obc8X1xOR1cSGj4ZXjy2th+BQzplceB43UM7R1qEthVWoPTIVR7/YzvX8gbGw5x8vDQqIud/mMM7ZVHWU09OW4npX1C+yzdeJiZw3qxsaQCl9NBeU09Tocwc1gvPt9/nPwcF7NH9mbJuhKCBvKynBQVZHPgWB2zRvTG7w/gLMjGXemlMMtJbo6LGkcVx6q9+IMGd8AwsGculXU+Jg4MDdOt8QUZ2COXPt1D1ZlB/Qz7y+tw9sljb2kNB47XMXVwDzbkNPz6d5XWMKIoNALjvS1HmD2id4P/GOG0spp6yqrrmTK4O9XeAO9vK2V4n3zG9S/gg+1HI8NRRxTls/5ABfvL6zh/bD/q/UHe2XyEgT1yGdo7j00HKzl9eGim7Of7jlNUmEOPXDer9pRz9sgiPt93nGy3k0MVHur8Ac4eXcSh7BPlOXCsjrXFFeRlOcnLcjFzeE8AjlaFhhVPHtSdkooTo9vW7q+gtMpLfSDIWaOL6Jbd8D99rtvFgB45lByoYPm2Uk4d1ouHvjCcZz7ei0OE/GwnDhHOn9SfQT3z+O2SzQ1Wff3uGcPx+gPsKq3hitOG8OB729l2uJpu2S6qvX6+fsogjlR5Ka3ycvaYIs44OfSgm79+dybfeeJTIDSi6vyJ/Vi97xjfnj2UB97bwWf7jjN9SA965GUxpFcehbluctwOth2qotLjpyDHRf/uucwZE1p6/O6vTGTiwO4Nzu2WL4/l3je2cM64kyit8nLySQVMG9KDCQMaPqcC4OXrT+ezfcc4UuXl308dHEn/wRdPJhA0DdIeuGIahbnxJ7hdP2ck9YEgl8+M/f/3vssm8/gHu/nVheNwOgWPL0BelpPhfbpF+h+KCrL52dzRXBQ10uq3X51EledEc+AL187mnjc2M6ZvAWeNLsIhQq/8LP707jaG98nnzFFFrNl7jB/828lMHtSdooLYy4aM7tuNG780ivlR595aPzl3NE6H8PVTBjO8dz6HqzzkZ7v4z3NGs/5ABdfNGcnZY05i++Gq+AdLMUlRKw8iMhu40xgzz3p/K4Ax5p6oPEutPCtExAUcAopiNT3NmDHDrFq1KiVlVkqpzkpEVhtjZrRk31SOxVoJjBKR4SKSBVwOLGqUZxHwHev1ZcB7qeqfUEop1TIpa3qy+hx+CCwlNDz2CWPMRhG5C1hljFkEPA48IyI7gHJCwUQppVQGSekwDWPM68DrjdJuj3rtAb6eyjIopZRqncycBqiUUipjaKBQSikVkwYKpZRSMWmgUEopFZMGCqWUUjGlbMJdqohIKbC3hbv3IQXLg6RZZzunznY+oOfUEXS284Gm5zTUGFPUXOZYOlygaA0RWdXSmYmZqrOdU2c7H9Bz6gg62/lA256TNj0ppZSKSQOFUkqpmLpaoHgs3QVIgc52Tp3tfEDPqSPobOcDbXhOXaqPQimlVPK6Wo1CKaVUkrpMoBCR80Rkq4jsEJFb0l2eRIjIYBFZJiKbRWSjiNxopfcSkbdFZLv1b08rXUTkz9Y5rhOR6ek9g+aJiFNEPhORxdb74SLyiXVOC62l6RGRbOv9Dmv7sHSW246I9BCRl0Rki/Vdze7o35GI/Kf1N7dBRJ4TkZyO9h2JyBMickRENkSlJf29iMh3rPzbReQ7dp/VXpo5p/utv711IvJ3EekRte1W65y2isi8qPTkrofGmE7/Q2iZ853ACCALWAuMT3e5Eih3f2C69boA2AaMB+4DbrHSbwF+Z70+H3iD0JMDZwGfpPscYpzb/wP+Biy23r8AXG69fgS43np9A/CI9fpyYGG6y25zLn8Fvme9zgJ6dOTviNAjincDuVHfzVUd7TsCzgKmAxui0pL6XoBewC7r357W654Zdk5zAZf1+ndR5zTeutZlA8Ota6CzJdfDtH+Z7fTLnQ0sjXp/K3BrusvVgvP4B3AusBXob6X1B7Zarx8FrojKH8mXST/AIOBd4N+AxdZ/zqNRf+yR74vQ80xmW69dVj5J9zlEnUuhdVGVRukd9jvixLPse1m/88XAvI74HQHDGl1Uk/pegCuAR6PSG+TLhHNqtO2rwALrdYPrXPh7asn1sKs0PYX/8MOKrbQOw6rOTwM+AfoaYw4CWP+eZGXrKOf5R+DnQNB63xs4bowJP4A6utyRc7K2V1j5M8UIoBR40mpK+z8RyacDf0fGmAPA74F9wEFCv/PVdNzvKFqy30vGf1+NfJdQzQja8Jy6SqAQm7QOM9xLRLoBLwM/McZUxspqk5ZR5ykiFwJHjDGro5NtspoEtmUCF6GmgIeNMdOAGkJNGs3J9PPBare/hFBzxQAgH/iyTdaO8h0lorlz6DDnJiK/BPzAgnCSTbYWnVNXCRTFwOCo94OAkjSVJSki4iYUJBYYY16xkg+LSH9re3/giJXeEc7zDOBiEdkDPE+o+emPQA8RCT9xMbrckXOytncn9NjcTFEMFBtjPrHev0QocHTk7+gcYLcxptQY4wNeAU6n435H0ZL9XjrC94XVyX4h8E1jtSfRhufUVQLFSmCUNWoji1CH26I0lykuERFCzxXfbIz5Q9SmRUB49MV3CPVdhNO/bY3gmAVUhKvZmcIYc6sxZpAxZhih7+E9Y8w3gWXAZVa2xucUPtfLrPwZc0dnjDkE7BeRMVbSl4BNdODviFCT0ywRybP+BsPn1CG/o0aS/V6WAnNFpKdV05prpWUMETkPuBm42BhTG7VpEXC5NSptODAK+JSWXA/T3dnUjh1A5xMaNbQT+GW6y5Ngmb9AqEq4Dvjc+jmfUPvvu8B2699eVn4BHrLOcT0wI93nEOf85nBi1NMI6494B/AikG2l51jvd1jbR6S73DbnMRVYZX1PrxIaHdOhvyPg18AWYAPwDKGRMx3qOwKeI9TH4iN0F/0fLfleCLX777B+rs7Ac9pBqM8hfI14JCr/L61z2gp8OSo9qeuhzsxWSikVU1dpelJKKdVCGiiUUkrFpIFCKaVUTBoolFJKxaSBQimlVEwaKFSXISIBEfk86ifmqpkicp2IfLsNPnePiPRpwX7zROROawz/660th1It5YqfRalOo84YMzXRzMaYR1JZmAScSWiS21nAR2kui+rCNFCoLs9aTmQh8EUr6RvGmB0icidQbYz5vYj8GLiO0Fo6m4wxl4tIL+AJQhPRaoFrjDHrRKQ3oYlRRYQmoEnUZ30L+DGh5Z0/AW4wxgQalWc+oRU9RxBac6kvUCkipxljLk7F70CpWLTpSXUluY2anuZHbas0xswEHiS09lRjtwDTjDGTCQUMCM1e/sxK+wXwtJV+B/ChCS0SuAgYAiAi44D5wBlWzSYAfLPxBxljFnLimQOTCM2OnqZBQqWL1ihUVxKr6em5qH//x2b7OmCBiLxKaJkOCC2xcimAMeY9EektIt0JNRV9zUpfIiLHrPxfAk4BVoaWUCKXE4vSNTaK0PIKAHnGmKoEzk+plNBAoVSIaeZ12AWEAsDFwG0iMoHYyzXbHUOAvxpjbo1VEBFZBfQBXCKyCegvIp8DPzLGfBD7NJRqe9r0pFTI/Kh/V0RvEBEHMNgYs4zQA5d6AN2A97GajkRkDnDUhJ4XEp3+ZUKLBEJoEbrLROQka1svERnauCDGmBnAEkL9E/cRWrRtqgYJlS5ao1BdSa51Zx72pjEmPEQ2W0Q+IXTzdEWj/ZzAs1azkgD/Y4w5bnV2Pyki6wh1ZoeXr/418JyIrAH+SWjZbowxm0TkV8BbVvDxAT8A9tqUdTqhTu8bgD/YbFeq3ejqsarLs0Y9zTDGHE13WZTKRNr0pJRSKiatUSillIpJaxRKKaVi0kChlFIqJg0USimlYtJAoZRSKiYNFEoppWLSQKGUUiqm/w9d9vaPqSgW9AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7faa2b1550b8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "plt.plot(np.arange(1, len(scores_max_hist)+1), scores_max_hist, label='score')\n",
    "plt.plot(np.arange(1, len(scores_mean_hist)+1), scores_mean_hist, label='average score')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_max_hist_test = []\n",
    "scores_mean_hist_test = []\n",
    "def maddpg_test(n_episodes=100):\n",
    "    \n",
    "    for i_episode in range(n_episodes):\n",
    "        env_info = env.reset(train_mode=False)[brain_name]\n",
    "        state = env_info.vector_observations\n",
    "        scores = np.zeros(num_agents)\n",
    "        maddpg.reset()\n",
    "        \n",
    "        while True:\n",
    "            action = maddpg.act(state, i_episode, add_noise=False)\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            \n",
    "            next_state = env_info.vector_observations\n",
    "            reward = env_info.rewards\n",
    "            done = env_info.local_done\n",
    "            \n",
    "            scores += reward\n",
    "            #print(i_episode)\n",
    "            #maddpg.step(i_episode, state, action, reward, next_state, done)\n",
    "            #print(reward)\n",
    "            if np.any(done):\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "            \n",
    "        score_max = np.max(scores)\n",
    "        score_mean = np.mean(scores_deque)\n",
    "        \n",
    "        scores_max_hist_test.append(score_max)\n",
    "        scores_mean_hist_test.append(score_mean)\n",
    "        \n",
    "        print('\\r{} episode\\tmoving avg score {:.5f}\\tmax score {:.5f}'.format(i_episode, np.mean(scores_deque), score_max), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, agent in enumerate(maddpg.agents):\n",
    "    chk_actor_filename = torch.load('checkpoint_agent{}_actor.pth'.format(idx))\n",
    "    chk_critic_filename = torch.load('checkpoint_critic{}_critic.pth'.format(idx))\n",
    "    agent.actor_local.load_state_dict(torch.load('checkpoint_agent{}_actor.pth'.format(idx)))\n",
    "    agent.critic_local.load_state_dict(torch.load('checkpoint_critic{}_critic.pth'.format(idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2889: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/conda/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.1450000023469329\n",
      "Total score (averaged over agents) this episode: 2.600000038743019\n",
      "Total score (averaged over agents) this episode: 2.595000038854778\n",
      "Total score (averaged over agents) this episode: 2.600000038743019\n",
      "Total score (averaged over agents) this episode: 2.650000039488077\n",
      "Total score (averaged over agents) this episode: 2.550000037997961\n",
      "Total score (averaged over agents) this episode: 2.650000039488077\n",
      "Total score (averaged over agents) this episode: 2.600000038743019\n",
      "Total score (averaged over agents) this episode: 0.04500000085681677\n",
      "Total score (averaged over agents) this episode: 0.09500000160187483\n",
      "Total score (averaged over agents) this episode: 1.5450000232085586\n",
      "Total score (averaged over agents) this episode: 0.04500000085681677\n",
      "Total score (averaged over agents) this episode: 0.6450000097975135\n",
      "Total score (averaged over agents) this episode: 2.600000038743019\n",
      "Total score (averaged over agents) this episode: 2.600000038743019\n",
      "Total score (averaged over agents) this episode: 2.650000039488077\n",
      "Total score (averaged over agents) this episode: 2.1950000328943133\n",
      "Total score (averaged over agents) this episode: 2.600000038743019\n",
      "Total score (averaged over agents) this episode: 2.650000039488077\n",
      "Total score (averaged over agents) this episode: 1.3450000202283263\n",
      "Total score (averaged over agents) this episode: 1.5950000239536166\n",
      "Total score (averaged over agents) this episode: 2.600000038743019\n",
      "Total score (averaged over agents) this episode: 2.650000039488077\n",
      "Total score (averaged over agents) this episode: 2.600000038743019\n",
      "Total score (averaged over agents) this episode: 0.44500000681728125\n",
      "Total score (averaged over agents) this episode: 0.3450000053271651\n",
      "Total score (averaged over agents) this episode: 2.600000038743019\n",
      "Total score (averaged over agents) this episode: 2.650000039488077\n",
      "Total score (averaged over agents) this episode: 2.600000038743019\n"
     ]
    }
   ],
   "source": [
    "scores_max_hist_test = []\n",
    "scores_mean_hist_test = []\n",
    "scores_deque = deque(maxlen=100)\n",
    "\n",
    "for i in range(100):                                         # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    maddpg.reset()\n",
    "    while True:\n",
    "        #actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "       # actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        #print(scores)\n",
    "        actions = maddpg.act(states, i, add_noise=False)\n",
    "        #print(\"Action: \",actions)\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "            \n",
    "        score_max = np.max(scores)\n",
    "        score_mean = np.mean(scores_deque)\n",
    "        \n",
    "        scores_max_hist_test.append(score_max)\n",
    "        scores_mean_hist_test.append(score_mean)\n",
    "    print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
